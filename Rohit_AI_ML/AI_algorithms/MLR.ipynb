{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Data Pre-processing Step:\n",
    "\n",
    "The very first step is data pre-processing, which we have already discussed in this tutorial. This process contains the below steps:\n",
    "\n",
    "### Importing libraries:\n",
    "\n",
    " Firstly we will import the library which will help in building the model. Below is the code for it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries  \n",
    "import numpy as nm  \n",
    "import matplotlib.pyplot as mtp  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset:\n",
    "\n",
    "Now we will import the dataset(50_CompList), which contains all the variables. Below is the code for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing datasets  \n",
    "data_set= pd.read_csv(\"multiple_linear_regression_dataset.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>30450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>35670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>31580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>40130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>47830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  experience  income\n",
       "0   25           1   30450\n",
       "1   30           3   35670\n",
       "2   47           2   31580\n",
       "3   32           5   40130\n",
       "4   43          10   47830"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above output, we can clearly see that there are five variables, in which four variables are continuous and one is categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting dependent and independent Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Independent and dependent Variable  \n",
    "x= data_set.iloc[:, :-1].values  \n",
    "y= data_set.iloc[:, 2].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output, the last column contains categorical variables which are not suitable to apply directly for fitting the model. So we need to encode this variable.\n",
    "\n",
    "### Encoding Dummy Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have one categorical variable (State), which cannot be directly applied to the model, so we will encode it. To encode the categorical variable into numbers, we will use the `LabelEncoder` class. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will use `OneHotEncoder`, which will create the dummy variables. Below is code for it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Methods: Label Encoding vs. One-Hot Encoding\n",
    "\n",
    "## 1. Label Encoding\n",
    "- **What It Does**: Converts each category into a numeric label (e.g., `0, 1, 2, ...`).\n",
    "- **Example**:  \n",
    "  - Categories: `[\"Red\", \"Blue\", \"Green\"]`  \n",
    "  - Encoded: `[0, 1, 2]`\n",
    "- **Usage**:  \n",
    "  - Useful when the categorical variable has an **ordinal relationship** (e.g., `Low < Medium < High`).\n",
    "  - **Caution**: Introduces **implicit order**, which might not make sense for **nominal categories** like `State` or `Color`.\n",
    "\n",
    "## 2. One-Hot Encoding\n",
    "- **What It Does**: Converts each category into a separate binary column (dummy variables).\n",
    "- **Example**:  \n",
    "  - Categories: `[\"Red\", \"Blue\", \"Green\"]`  \n",
    "  - Encoded:  \n",
    "    ```\n",
    "    [[1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [0, 0, 1]]\n",
    "    ```\n",
    "- **Usage**:  \n",
    "  - Suitable for **nominal categories** (no inherent order), such as `State` or `City`.\n",
    "  - Avoids introducing **artificial relationships** between categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply One-Hot Encoding to the 'State' column (assumed to be at index 3)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m column_transformer \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m      6\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      7\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(), [\u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# Apply OneHotEncoder to column index 3\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     ],\n\u001b[0;32m      9\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Keep other columns as they are\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m column_transformer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mx\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Apply One-Hot Encoding to the 'State' column (assumed to be at index 3)\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), [3])  # Apply OneHotEncoder to column index 3\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as they are\n",
    ")\n",
    "\n",
    "x = column_transformer.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are only encoding one independent variable, which is state as other variables are continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dummy Variables and the Dummy Variable Trap\n",
    "\n",
    "When using **One-Hot Encoding**, categorical variables are converted into multiple binary columns (dummy variables), each representing a category. Here's a detailed explanation based on the given scenario:\n",
    "\n",
    "### Dummy Variable Encoding Output\n",
    "After encoding the `State` column, each unique state is represented as a binary column:\n",
    "- **California**: `[1, 0, 0]`\n",
    "- **Florida**: `[0, 1, 0]`\n",
    "- **New York**: `[0, 0, 1]`\n",
    "\n",
    "### Correspondence of Columns:\n",
    "- The **first column** corresponds to `California`.\n",
    "- The **second column** corresponds to `Florida`.\n",
    "- The **third column** corresponds to `New York`.\n",
    "\n",
    "This allows the model to differentiate between states using binary values. However, if we include **all three dummy variables**, it introduces a redundancy, as the values in the third column can be derived from the first two. This redundancy leads to the **dummy variable trap**.\n",
    "\n",
    "---\n",
    "\n",
    "### What is the Dummy Variable Trap?\n",
    "The **dummy variable trap** occurs when:\n",
    "1. All dummy variables are included in the model, causing **multicollinearity** (high correlation between variables).\n",
    "2. The model becomes unable to uniquely determine the effect of each variable because one variable can be expressed as a linear combination of others.\n",
    "\n",
    "For example:\n",
    "- If `California = 1`, `Florida = 0`, it implies `New York = 0`.  \n",
    "Thus, one column is always dependent on the others, leading to redundant information.\n",
    "\n",
    "---\n",
    "\n",
    "### Avoiding the Dummy Variable Trap\n",
    "To avoid this issue:\n",
    "1. **Exclude one dummy variable**: This prevents multicollinearity by removing the redundant column.\n",
    "2. **Interpretation**: The excluded column becomes the **baseline category**, and the remaining dummy variables compare against it.\n",
    "\n",
    "For the given dataset:\n",
    "- Remove the first column (`California`), so the dataset retains only:\n",
    "  - Florida: `[1, 0]`\n",
    "  - New York: `[0, 1]`\n",
    "- California is now implicitly represented when both dummy variables are `0`.\n",
    "\n",
    "### Code to Avoid the Dummy Variable Trap:\n",
    "```python\n",
    "x = x[:, 1:]  # Exclude the first column to avoid the trap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we are writing a single line of code just to avoid the dummy variable trap:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoiding the dummy variable trap:  \n",
    "x = x[:, 1:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 1.0 86419.7 153514.11 0.0]\n",
      " [0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [0.0 0.0 0.0 135426.92 0.0]\n",
      " [0.0 1.0 542.05 51743.15 0.0]\n",
      " [0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above output image, the first column has been removed.\n",
    "\n",
    "**Now we will split the dataset into training and test set. The code for this is given below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not remove the first dummy variable, then it may introduce multicollinearity in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set.  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step: 2- Fitting our MLR model to the Training set:\n",
    "Now, we have well prepared our dataset in order to provide training, which means we will fit our regression model to the training set. It will be similar to as we did in Simple Linear Regression model. The code for this will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression  \n\u001b[0;32m      3\u001b[0m regressor\u001b[38;5;241m=\u001b[39m LinearRegression()  \n\u001b[1;32m----> 4\u001b[0m regressor\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m, y_train)  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Fitting the MLR model to the training set:  \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor= LinearRegression()  \n",
    "regressor.fit(x_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 3- Prediction of Test set results:\n",
    "\n",
    "The last step for our model is checking the performance of the model. We will do it by predicting the test set result. For prediction, we will create a y_pred vector. Below is the code for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the Test set result;  \n",
    "y_pred= regressor.predict(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.9501847627493607\n",
      "Test Score:  0.9347068473282966\n"
     ]
    }
   ],
   "source": [
    "print('Train Score: ', regressor.score(x_train, y_train))  \n",
    "print('Test Score: ', regressor.score(x_test, y_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above score tells that our model is 95% accurate with the training dataset and 93% accurate with the test dataset.\n",
    "\n",
    "`Note: In the next topic, we will see how we can improve the performance of the model using the Backward Elimination process.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
